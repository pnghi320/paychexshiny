---
title: "Paychex_LSTM_RNN_Rcode"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(reticulate)
library(tensorflow)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(tidyverse)
library(svDialogs)

```

```{python}
import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
#import seaborn as sns # used for plot interactive graph. 
from sklearn.metrics import mean_squared_error,r2_score
## Deep-learing:
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Dense, Input, Multiply, Concatenate
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
import itertools
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dropout
#import IPython
import matplotlib as mpl
import os
import pickle
#########################################################################################################
#********************THIS WILL BE UPDATED SHORTLY TO PULL DIRECTLY FROM DATAVERSE***********************#
#########################################################################################################
#df2 = pd.read_csv (r"C:\Users\Tyler\Desktop\Newfolder2\PayChex\Data\PayChex_Data_convTimstamp_Date123.csv")
#data = pd.read_csv(r"C:\Users\Tyler\Desktop\Newfolder2\PayChex\Data\PayChex_Data_convTimstamp_Date123.csv",parse_dates={'dt' : ['dates']}, infer_datetime_format=True, low_memory=False, na_values=['nan','?'], index_col='dt')
  
  
                 
#Get the data for metrics of interest
# mask = np.logical_or(data['metricName'] == 'Calls per Minute', data['metricName'] == 'Average Response Time (ms)')
# data_masked = data[mask]
# date_range = pd.date_range(data_masked.index.min(), data_masked.index.max(), freq='H')
# data_masked = data_masked.drop(columns=['busApp', 'metricFrequency','days','hours','numericdays','timeinms'])
def loadData(dfraw):
    #"""Takes data from dataverse as a dataframe, cleans up missing values, rearranges into row by date, expands variables.
    #Output columns are the metric/service combinations followed by missing_columns that denote data which needed to be filled in
    #and the expanded date columns.  Also, returns the number of metric per service columns.
    #"""
    #Get the data for metrics of interest
    numMetrics = 2 #hard coded to take calls per minute & avg response time
    mask = np.logical_or(dfraw['metricName'] == 'Calls per Minute', dfraw['metricName'] == 'Average Response Time (ms)')
    data_masked = dfraw[mask]
    date_range = pd.date_range(data_masked.index.min(), data_masked.index.max(), freq='H')
    data_masked = data_masked.drop(columns=['busApp', 'metricFrequency','days','hours','numericdays','timeinms'])
    #Expand data frame to all date-metric combinations, leaving N/A for missing data 
    data_masked = data_masked.rename_axis('dt').reset_index()
    index_full = pd.MultiIndex.from_product([date_range, data_masked.metricName.unique(), data_masked.serviceName.unique()], names=['dt', 'metricName', 'serviceName'])
    df_full = data_masked.set_index(['dt', 'metricName','serviceName']).reindex(index_full).reset_index()
    #Determine what services to use in model, currently all being utilized
    #good_service is the list of services to include 
    #(We are assuming all services can be used but could be edited in the future)
    valServiceCounts = df_full.groupby('serviceName').count()
    good_service = valServiceCounts.index
    df_subset = df_full[df_full['serviceName'].isin(good_service)]
    #Set all N/A values to 0, and add missing_column to denote real vs added 0 values
    sortedGood = df_subset.pivot_table(index ='dt',columns =['metricName','serviceName'],values=['value'] ) 
    missingdata = sortedGood.iloc[:,:len(good_service)].isna().astype(int)
    missingdata.columns = ['missing_'+col for col in good_service]
    sortedGood = pd.concat([sortedGood,missingdata],axis = 1).fillna(0)
    #Take log of all variables
    sortedGood.iloc[:,:2*len(good_service)] = np.log(sortedGood.iloc[:,:2*len(good_service)]+1)
    sortedGood = sortedGood.reset_index()
    #Dummification of dt 
    sortedGood['Hour']= sortedGood['dt'].dt.hour
    sortedGood['DayofWeek']= sortedGood['dt'].dt.dayofweek
    sortedGood['Month']= sortedGood['dt'].dt.month
    sortedGood = pd.get_dummies(sortedGood,columns = ['Hour','DayofWeek','Month'])
    results = [sortedGood,len(good_service),numMetrics]
    return results


df2 = pd.read_csv (r"/Users/tranquynhphuongnghi/Desktop/LSTM/PayChex_Data_convTimstamp_Date123.csv")
data = pd.read_csv(r"/Users/tranquynhphuongnghi/Desktop/LSTM/PayChex_Data_convTimstamp_Date123.csv", 
                 parse_dates={'dt' : ['dates']}, infer_datetime_format=True, 
                 low_memory=False, na_values=['nan','?'], index_col='dt')
  
#Function break comment

def dfXform(df,numMetrics,numServices,chunk_length, scaler):
    #From master_df grab the following:
    # X is the values of dt & metrics used for prediction
    # Y is the values of metrics to predict
    # Z is binary record of which 0 values (in Y) are real or added, 1 = real 0 = fake
    # T is dummified time values
    # raw_dt is human readable time (not used in the model)
    numout = numMetrics*numServices
    scaled = df.copy()  # not actually scaled! (because the line below does scaling and is commented out)
    # scaled.iloc[:,1:1+numout] = scaler.transform(scaled.iloc[:,1:1+numout]) 
    raw_dt = scaled['dt'].values[1:] # raw date column
    X = scaled.values[:-1,1:] # all columns except raw date
    Y = scaled.values[1:,1:1+numout] # all service/metric columns
    Z = 1-scaled.values[1:,1+numout:(1+numout+numServices)] # all missing_ columns
    T = scaled.values[1:,1+numout+numServices:] # all expanded time columns
    num_chunks = int(np.floor(len(X)/chunk_length))
    extra_points = len(X)-num_chunks*chunk_length
    if extra_points != 0:
        print("Input data did not divide into an equal number of chunks, dropping oldest {} points".format(extra_points))
    X = X[extra_points:].reshape(num_chunks,chunk_length,-1).astype(float)
    Y = Y[extra_points:].reshape(num_chunks,chunk_length,-1).astype(float)
    Z = Z[extra_points:].reshape(num_chunks,chunk_length,-1).astype(float)
    T = T[extra_points:].reshape(num_chunks,chunk_length,-1).astype(float)
    raw_dt = raw_dt[extra_points:].reshape(num_chunks,chunk_length,-1).astype(float)
    X_last = scaled.values[-chunk_length:,1:].reshape(1,chunk_length,-1).astype(float)
    raw_dt_last = scaled['dt'].values[-chunk_length:].reshape(1,chunk_length,-1).astype(float)
    return (X, Z, T, raw_dt, Y),(X_last, raw_dt_last)
    

#Expand data frame to all date-metric combinations, leaving N/A for missing data 
chunk_length = 100
num_chunks = 16
num_train_chunks = 12

num_test_chunks = num_chunks - num_train_chunks

scalerFile = 'Scaler.pkl'
results = loadData(data)
df_parsed = results[0]
numServices = results[1]
numMetrics= results[2]
numout = numMetrics*numServices
scaler = pickle.load(open(scalerFile,'rb'))
assert len(scaler.scale_) == numMetrics*numServices
parsedData,lastHourData = dfXform(df_parsed,numMetrics,numServices,chunk_length, scaler)     
X, Z, T, raw_dt, Y = parsedData

print(df_parsed)
print(numServices)
print(numMetrics)

#Populating list of metrics
listMetrics = []
for i in range(numMetrics):
  listMetrics.append(df_parsed.columns[1+i*numServices][1])
print(listMetrics)


#Randomly divide chunks by train and test
index= np.arange(num_chunks)
np.random.seed(42)
np.random.shuffle(index)
X_train = X[index[:num_train_chunks]]
Y_train = Y[index[:num_train_chunks]]
X_test = X[index[num_train_chunks:]]
Y_test = Y[index[num_train_chunks:]]
#X_final = X_final.reshape(1,chunk_length,-1).astype(float)
dt_train = raw_dt[index[:num_train_chunks]]
dt_test = raw_dt[index[num_train_chunks:]]
Z_train = Z[index[:num_train_chunks]]
Z_test = Z[index[num_train_chunks:]]
T_train = T[index[:num_train_chunks]]
T_test = T[index[num_train_chunks:]]


X_trainflat = X_train[:,:,:numout].reshape(num_train_chunks*chunk_length,-1)
Y_trainflat = Y_train.reshape(num_train_chunks*chunk_length,-1)
X_testflat = X_test[:,:,:numout].reshape(num_test_chunks*chunk_length,-1)
Y_testflat = Y_test.reshape(num_test_chunks*chunk_length,-1)

#Scaling
from sklearn.preprocessing import MinMaxScaler

Scaler = MinMaxScaler()
Scaler.fit(X_trainflat)
pickle.dump(Scaler,open('Scaler.pkl','wb'))
X_trainflat = Scaler.transform(X_trainflat)
X_testflat = Scaler.transform(X_testflat)
Y_trainflat = Scaler.transform(Y_trainflat)
Y_testflat = Scaler.transform(Y_testflat)

X_train[:,:,:numout] = X_trainflat.reshape([-1,chunk_length,numout])
X_test[:,:,:numout] = X_testflat.reshape([-1,chunk_length,numout])


Y_train = Y_trainflat.reshape(Y_train.shape)
Y_test = Y_testflat.reshape(Y_test.shape)


#Define model architecture
#Compile Model (when run, resets variables)

def runModel(hidden_dim,epochs,x_train,y_train,z_train,x_test,y_test,z_test):
    n_input = x_train.shape[-1]
    # hidden_dim = 100
    #input feeds rnn
    input_ = Input(shape=(chunk_length, n_input))
    pre = Dense(2*hidden_dim, activation='relu')(input_)
    dropout = Dropout(0.2)(pre)
    rnn = LSTM(hidden_dim, return_sequences=True) (dropout)
    post = Dense(hidden_dim, activation='relu')(rnn)
    pred = Dense(numout)(post) 
    #pred = Dense(numout,activation='sigmoid')(post) 
    z_in = Input(shape=(chunk_length,numServices))
    zd_in = Concatenate()([z_in,z_in])
    output = Multiply()([pred,zd_in])#masking 0
    model = Model(inputs=[input_,z_in], outputs=output)
    print(model.summary())
    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
    model.compile(loss='mse', optimizer=opt,metrics = [])
    history = model.fit([x_train,z_train], y_train, epochs = epochs, validation_data = ([x_test, z_test],y_test))
    return history,model

#dims = [10,20,40,80,160,320]
dims = [10,20]
allHist = []

for hidden_dim in dims:
    history,_ = runModel(hidden_dim,200,X_train,Y_train,Z_train,X_test,Y_test,Z_test)
    allHist.append(history)

for hidden_dim ,history in zip(dims,allHist):
    plt.plot(history.history['val_loss'])
plt.legend(dims)    
plt.show()
plt.clf()

for hidden_dim ,history in zip(dims,allHist):
    plt.plot(history.history['loss'])
plt.legend(dims)
plt.show()
plt.clf()


history,model = runModel(100,500,X_train,Y_train,Z_train,X_test,Y_test,Z_test)

model.save('LSTM_Model')

pred1 = ((model.predict([X_test,Z_test])[:,-1,:]))
pd.DataFrame(data = pred1, columns = df_parsed.columns[1:numout+1])

#Show scaled true values on test data set
actual = ((Y_test[:,-1,:]))
pd.DataFrame(data = actual, columns = df_parsed.columns[1:numout+1])

#populates the unique service list for Rshiny
listServices = [el[2] for el in df_parsed.columns[1:1+numServices]]
listServices
#RMSE Report
#Each row is a service/metric combination, the value is the RMSE of the scaled values, 
#and the final int is the # of real data points (only valid points contributed to the RMSE values)

##############################

preds = model.predict([X_test,Z_test]) 

error = 0
count = 0

rmseReport = []

for i in range(2*numServices):
  pp = preds[:,:,i].reshape(-1)
  aa = Y_test[:,:,i].reshape(-1)
  real_data_index = Z_test[:,:,i%numServices].reshape(-1).astype(bool)
  pp = pp[real_data_index]
  aa = aa[real_data_index]
  error += np.square(pp-aa).sum()
  count += len(pp)
  rmse = np.sqrt(np.square(pp-aa).sum()/len(pp))
  if i < numServices:
    metric = 'Average Response Time (ms)'
  else:
    metric = 'Calls per Minute'
  rmseReport.append((i,listServices[i%numServices],metric, rmse,len(pp)))

rmseReport = pd.DataFrame(data = rmseReport,columns = ['origIndex','Service Name','metric','rmse','Valid Count'])
#Last line is the overall RMSE across all services/metrics


#Fraction of pts within .1 threshhold of scaled predictions

is_correct = (preds - Y_test) < 0.1
is_correct = is_correct.reshape(-1)
accuracy = np.sum(is_correct)/len(is_correct)
print(accuracy)


X_last, raw_dt_last = lastHourData #Z_last, T_last, raw_dt_last = lastHourData
X_last_scaled = X_last.copy()
X_lastflat = X_last[:,:,:numout].reshape(1*chunk_length,-1)
X_lastflat_scaled = Scaler.transform(X_lastflat)
X_last_scaled[:,:,:numout] = X_lastflat_scaled.reshape([-1,chunk_length,numout])
X_last_scaled.max()


predLast = model.predict([X_last_scaled,np.ones((1,chunk_length,numServices))])
unscaledpred = scaler.inverse_transform(predLast[:,-1,:])[0]
previousHour = (X_last_scaled[:,-1,:numMetrics*numServices]) #scaled
actual = scaler.inverse_transform(previousHour)[0]
predReal = np.exp(unscaledpred)-1
actualReal = np.exp(actual)-1


rmseReport['Previous Hour Scaled Value']=previousHour[0]
rmseReport['Next Hour Scaled Prediction']=predLast[:,-1,:][0]
rmseReport['Previous Hour Value']=actualReal
rmseReport['Next Hour Prediction']=predReal
rmseReport
rmseReport.iloc[-5]


train_preds = model.predict([X_train,Z_train])
test_preds = model.predict([X_test,Z_test])

def plot_by_index(serviceName, metricName, col_id_num, chunk_id=-1, train_data=True):
    #set chunk_id to -1 to always pull most recent chunk 
    if train_data:
      plt.plot(np.arange(chunk_length),Y_train[chunk_id,:,col_id_num])
      plt.plot(np.arange(chunk_length),model.predict([X_train,Z_train])[chunk_id,:,col_id_num])
    else:
      plt.plot(np.arange(chunk_length),Y_test[chunk_id,:,col_id_num])
      plt.plot(np.arange(chunk_length),model.predict([X_test,Z_test])[chunk_id,:,col_id_num])
    if col_id_num >= numServices:
        Z_index = col_id_num - numServices
    else:
        Z_index = col_id_num
    if train_data:
      plt.plot(np.arange(chunk_length),Z_train[chunk_id,:,Z_index])
    else:
      plt.plot(np.arange(chunk_length),Z_test[chunk_id,:,Z_index])
    plt.gca().legend(('real','predicted','True Data'))#,'Naive Model'))
    print(rmseReport.iloc[col_id_num])
    ##############Add color for data point (red)
    plt.scatter(chunk_length,predLast[0,-1,col_id_num],color='red')
    plt.show()
    fileName = serviceName+metricName+".png"
    plt.savefig(fileName)
    plt.clf()

col_id_num = 4

for i in range(12):
  plot_by_index(col_id_num, i)
for i in range(4):
  plot_by_index(col_id_num, i, False)  


last_chunk = np.argmax(index)
if last_chunk<num_train_chunks:
  plot_by_index(col_id_num, last_chunk)
else:
  plot_by_index(col_id_num, last_chunk-num_train_chunks, False)

```

```{python}

rmseReport["Service Name"].iloc[-1]
rmseReport["metric"].iloc[-1]
rmseReport["rmse"].iloc[-1]
rmseReport["Valid Count"].iloc[-1]
rmseReport["Previous Hour Scaled Value"].iloc[-1]
rmseReport["Next Hour Scaled Prediction"].iloc[-1]
rmseReport["Previous Hour Value"].iloc[-1]
rmseReport["Next Hour Prediction"].iloc[-1]
#print(rmseReport.shape)
print(rmseReport.tail())
for col in rmseReport.columns:
    print(col)

```



```{r}

#r/python to rshiny
serviceList <- as.list(py$listServices)
metricList <- as.list(py$listMetrics)
typeof(serviceList)

```


```{python}

last_chunk = np.argmax(index)
for metric in listMetrics:
  for service in listServices:
    service_id = listServices.index(service)
    metric_id = listMetrics.index(metric)
    col_id_num = service_id + metric_id*numServices
    print(col_id_num)
    if last_chunk<num_train_chunks:
      plot_by_index(service, metric,col_id_num, last_chunk)
    else:
      plot_by_index(service, metric,col_id_num, last_chunk-num_train_chunks, False)
```


```{python}

table = pd.DataFrame(columns = ['origIndex','Service Name','metric','rmse','Valid Count'])
for i in range(168):
  col_id_num = i
  rmseReport = []
  pp = preds[:,:,col_id_num].reshape(-1)
  aa = Y_test[:,:,col_id_num].reshape(-1)
  real_data_index = Z_test[:,:,col_id_num%numServices].reshape(-1).astype(bool)
  pp = pp[real_data_index]
  aa = aa[real_data_index]
  error += np.square(pp-aa).sum()
  count += len(pp)
  rmseuser = np.sqrt(np.square(pp-aa).sum()/len(pp))
  if col_id_num < numServices:
    metric = 'Average Response Time (ms)'
  else:
    metric = 'Calls per Minute'
  rmseReport.append((col_id_num,listServices[col_id_num%numServices],metric, rmseuser,len(pp)))
  rmseReport = pd.DataFrame(data = rmseReport,columns = ['origIndex','Service Name','metric','rmse','Valid Count'])
  #Last line is the overall RMSE across all services/metrics
  table = table.append(rmseReport, ignore_index=True)
  #sheet.append(pd.DataFrame([[1,2,3,4,2,1,1,1]]), ignore_index=True)
  #RMSE is the variable you would want to expose in R-shiny
  #might also be worth displaying the # of valid data points for the particular service which can be done with the following: len(pp) going to call variable valc
  #print(rmseuser)
  #valc = len(pp)
  #print(len(pp))
  #type(rmseReport)

```


```{r}
accuracy = py$accuracy
Rtable = py$table
Rtable <- data.frame(matrix(unlist(Rtable), nrow=168))
```


```{r}
#
# This is a Shiny web application. You can run the application by clicking
# the 'Run App' button above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
#

library(lubridate)
library(tidyverse)
library(svDialogs)
library(reticulate)
library(tensorflow)
library(shiny)
library(fpp2)
library(scales)
library(shiny)
library(datasets)
library(forecast)
library(shinyWidgets)



# Define UI for application that draws a histogram
ui <- fluidPage(
  tags$head(tags$style("#accuracy{
                                 font-size: 40px;
                                 text-align: center;
                                 display: block;
                                 padding-top: 60px;
                                 }"
                         ),
            tags$style("#overallAccuracy{
                                  font-size: 40px;
                                 text-align: center;
                                 display: block;
                                 padding-top: 60px;
                                 }"
                         ),
            tags$style("h1{
                                font-size: 50px;
                                 text-align: center;
                                 display: block;
                                 padding-top: 30px;
                                 }"
                         ),
              ),
    h1("Paychex Service Resource Analyser", align = "center"),
    br(),
    br(),
    # Sidebar with a slider input for number of bins 
    sidebarLayout(
        sidebarPanel(
            pickerInput("model", "Choose a model:",
                        list(`Machine-learning` = list("Choose one" = "","Tensorflow"),
                             `Time-series` = list("Choose one" = "","ARIMA", "ETS", "Seasonal naive"),
                             `Linear regression` = list("placeholder4", "placeholder5", "placeholder6"))
            ),
            pickerInput("service", "Choose a service:",c("Choose one" = "", "placeholder 1",
                                                         "placeholder 2",
                                                         "placeholder 3")),
            pickerInput("metric", "Choose a metric:",c("Choose one" = "", "placeholder 1",
                                                         "placeholder 2",
                                                         "placeholder 3")),
            #sliderInput(
            #    "time", "Hours to Forecast Ahead:",
            #    min = 0, max = 100,
            #    value = 1),
            #div(submitButton("Update View"), align = "right"),
        ),
        
        # Show plots
        mainPanel(
            tabsetPanel(
                #tabPanel("Exponetial Smoothing (ETS) Forecast", plotOutput("etsForecastPlot")), 
                tabPanel("Forecast Plot", plotOutput("forecastPlot")),
                #tabPanel("Timeseries Decomposition", plotOutput("dcompPlot")),
                #tabPanel('Accuracy Analysis', tableOutput("accuracy")),
                tabPanel('Root Mean Square Error (RMSE)',textOutput("accuracy")),
                tabPanel('Overall Accuracy',textOutput("overallAccuracy"))
            ),
            
        )
    )
)

# Define server logic required to draw a histogram
server <- function(input, output, session) {
    observeEvent(input$model, {
        if (input$model == "ARIMA"){
            x <- c("Choose one" = "", "Arimaservice1",
                   "Arimaservice2",
                   "Arimaservice3")
        }
        else if (input$model == "Tensorflow"){
          x <- serviceList
        }
        else if (input$model == "ETS"){
            x <- c("Choose one" = "", "ESTservice1",
                   "ESTservice2",
                   "ESTservice3")
        }
        else if (input$model == "Seasonal naive"){
            x <- c("Choose one" = "", "Seasonalservice1",
                   "Seasonalservice2",
                   "Seasonalservice3")
        }
        else{
            x <- character(0);
        }
        
        
        updatePickerInput(session = session, inputId = "service",
                          choices = x)
        
    }, ignoreInit = TRUE)
    
    observeEvent(input$service, {
        if (input$model == "Tensorflow"){
          y <- metricList
        }
        else if (input$service == "Arimaservice1"){
            y <- c("Choose one" = "", "Arimametric1",
                   "Arimametric2",
                   "Arimametric3")
        }
        else if (input$service == "Arimaservice2"){
            y <- c("Choose one" = "", "Arimametric4",
                   "Arimametric5",
                   "Arimametric6")
        }
        else if (input$service == "Arimaservice3"){
            y <- c("Choose one" = "", "Arimametric7",
                   "Arimametric8",
                   "Arimametric9")
        }
        else if (input$service == "ESTservice1"){
            y <- c("Choose one" = "", "ESTmetric1",
                   "ESTmetric2",
                   "ESTmetric3")
        }
        else if (input$service == "ESTservice2"){
            y <- c("Choose one" = "", "ESTmetric4",
                   "ESTmetric5",
                   "ESTmetric6")
        }
        else if (input$service == "ESTservice3"){
            y <- c("Choose one" = "", "ESTmetric7",
                   "ESTmetric8",
                   "ESTmetric9")
        }
        else if (input$service == "ESTservice1"){
            y <- c("Choose one" = "", "ESTmetric1",
                   "ESTmetric2",
                   "ESTmetric3")
        }
        else if (input$service == "ESTservice2"){
            y <- c("Choose one" = "", "ESTmetric4",
                   "ESTmetric5",
                   "ESTmetric6")
        }
        else if (input$service == "ESTservice3"){
            y <- c("Choose one" = "", "ESTmetric7",
                   "ESTmetric8",
                   "ESTmetric9")
        }
        else{
            y <- character(0);
        }
        
        # Method 1
        updatePickerInput(session = session, inputId = "metric",
                          choices = y)
         output$accuracy <-renderText({
      acc <- Rtable[Rtable$X3 == input$metric,]
      acc <- acc[acc$X2 == input$service,]
      acc1 = acc$X4
      paste("RMSE = ", substr(acc1, 1, 6), sep="")
    })
    #output$accuracy <- renderTable(accuracy(beerfit1, beer3))
    #output$table2 <- renderTable(summary(beerfit1))
    output$overallAccuracy <- renderText({
      paste("Overall accuracy = ", substr(accuracy, 1, 6), sep="")
    })
    }, ignoreInit = TRUE)

        #output$dcompPlot <- renderPlot({
    #    ds_ts <- ts(wineind, frequency=12)
    #    f <- decompose(ds_ts)
    #    plot(f)
    #})
    
    #output$arimaForecastPlot <- renderPlot({
        
        #fit <- auto.arima(wineind)
        #plot(forecast(fit, h=input$time))
   # })
    
    output$forecastPlot <- renderImage({
    # A temp file to save the output. It will be deleted after renderImage
    # sends it, because deleteFile=TRUE.
    outfile <- tempfile(fileext='.png')

    # Generate a png
    png(outfile, width=200, height=200)

    # Return a list
    fileName = paste(input$service, input$metric,".png", sep="")
    list(src = fileName,
         alt = "")
  }, deleteFile = FALSE)

    #beer2 <- window(ausbeer,start=1992,end=c(2007,4))
    #beerfit1 <- meanf(beer2,h=10)
    #beer3 <- window(ausbeer, start=2008)
   
    
}

# Run the application 
shinyApp(ui = ui, server = server)
 
```










